# -*- coding: utf-8 -*-
"""Crop_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zBBYtd9hM7vLsIKTpBrSDqnyWUyZKU2p

# CROP PREDICTION SYSTEM

Develop a machine learning model for crop yield prediction in Maharashtra, leveraging historical data of rainfall, temperature, area and production. The system aims to forecast crop yield by analyzing the correlation between the environmental factors and crop yield of previous years, contributing to informed agricultural decision-making, aiding farmers in making informed decisions for optimal crop cultivation.
"""

#importing required libraries
import pandas as pd
import numpy as np

#Loading dataset into dataframe
df = pd.read_csv('Final.csv')
df

district_mapping = df['Dist Name'].astype('category').cat.categories
crop_mapping = df['Crop'].astype('category').cat.categories

print("Available Districts:", list(district_mapping))
print("Available Crops:", list(crop_mapping))

print(df.columns.tolist())

from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# @title Area(1000 ha) vs Production(1000 tons)

from matplotlib import pyplot as plt
df.plot(kind='scatter', x='Area(1000 ha)', y='Production(1000 tons)', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

df = df.drop(df.columns[[0]], axis=1)

# checking null values in data
df.isnull()

# total null values present in dataset in specific columns
df.isnull().sum()

mean_value = df.select_dtypes(include=['number']).mean()
df.fillna(mean_value, inplace=True)

df['Crop'].value_counts()

df['Dist Name'].value_counts()

# replacing null values in total rain column with average rain
avg_rain = df['Total Rainfall'].mean()
df['Total Rainfall'].fillna(value=avg_rain, inplace=True)

# replacing null values in average temp column with average temperature
avg_temp = df['Avg Temp'].mean()
df['Avg Temp'].fillna(value=avg_temp, inplace=True)

# checking if any null values remain
df.isnull().sum()

# Returning shape of array
df.shape

# Returning descriptive statistics about the data
df.describe()

from sklearn.preprocessing import LabelEncoder

df_encoded = df.copy()
for col in df_encoded.select_dtypes(include=['object']).columns:
    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col])

df_encoded.corr()

# Visualizing the features
import seaborn as sns
ax = sns.pairplot(df, hue='Crop')
ax

sns.jointplot(x="Total Rainfall",y="Avg Temp",data=df[(df['Avg Temp']<40) & (df['Total Rainfall']>40)],height=10,hue="Crop")

import seaborn as sns
import matplotlib.pyplot as plt
numeric_df = df.select_dtypes(include=['number'])
plt.figure(figsize=(10, 6))
sns.heatmap(numeric_df.corr(), annot=True, cmap='magma')
plt.show()

# Creating dummy variables from categorical variables
dummy = pd.get_dummies(df)
dummy

# @title Splitting dataset into train and test dataset
from sklearn.model_selection import train_test_split

x = dummy.drop(["Yield(Kg per ha)"], axis=1)
y = dummy["Yield(Kg per ha)"]

# Splitting data set - 25% test dataset and 75%

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25, random_state=5)

print("x_train :",x_train.shape)
print("x_test :",x_test.shape)
print("y_train :",y_train.shape)
print("y_test :",y_test.shape)

print(y)

print(x_train)

# @title Linear regression
# Training the Simple Linear Regression model .

from sklearn.linear_model import LinearRegression
linear_model = LinearRegression()
linear_model.fit(x_train,y_train)

# Predicting Result
lr_predict = linear_model.predict(x_test)
lr_predict

linear_model.score(x_test,y_test)

# Calculating R2 score
from sklearn.metrics import r2_score
r = r2_score(y_test,lr_predict)
print("R2 score : ",r)

# Plotting scatterplot
plt.scatter(y_test,lr_predict, color='green')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Linear Regression')

ax = sns.kdeplot(y_test, color = "r", label = 'Actual Values')
sns.kdeplot(lr_predict, color = "green", label = 'Predicted Values', ax = ax)
plt.legend()
plt.title('Linear Regression')

import numpy as np
from sklearn import linear_model
from sklearn import svm
from sklearn.metrics import r2_score

classifiers = [
    linear_model.BayesianRidge(),
    linear_model.LassoLars(),
    linear_model.ARDRegression(),
    linear_model.TheilSenRegressor(),
    linear_model.LinearRegression()]

for clf in classifiers:
    print(clf)
    clf.fit(x_train, y_train)

    # Predicting on the test set
    y_pred = clf.predict(x_test)

    # Calculating R^2 score
    r2 = r2_score(y_test, y_pred)

    print("R^2 Score:", r2)
    print('\n')

from sklearn.ensemble import RandomForestRegressor

# ✅ Optimized Random Forest
random_model = RandomForestRegressor(
    n_estimators=20,    # fewer trees (try 20 first)
    max_depth=12,       # limits tree depth (faster + avoids overfitting)
    random_state=42,    # reproducibility
    n_jobs=-1           # use all CPU cores
)

# Fit on training data
random_model.fit(x_train, y_train)

print("✅ Model trained successfully!")

# Calculating R2 score

from sklearn.metrics import r2_score
r1 = r2_score(y_test,rf_predict)
print("R2 score : ",r1)

# Calculating Adj. R2 score:

Adjr2_1 = 1 - (1-r)*(len(y_test)-1)/(len(y_test)-x_test.shape[1]-1)
print("Adj. R-Squared : {}".format(Adjr2_1))

ax = sns.kdeplot(y_test, color = "r", label = 'Actual Values')
sns.kdeplot(rf_predict, color = "green", label = 'Predicted Values', ax = ax)
plt.legend()
plt.title('Random Forest Regression')

"""###Comparison between Linear Regression Algorithm and Random Forest Algorithm

Linear regression algorithm is not at all accurate for this kind of prediction.
Random Forest Algorithm has higher accuracy ( between 95 % to 99% ), but it is slow.
"""

# @title Decision Tree
# Training model
from sklearn.tree import DecisionTreeRegressor
dec_model = DecisionTreeRegressor(random_state = 14)
dec_model.fit(x_train,y_train)

# Predicting results
decisiontree_predict = dec_model.predict(x_test)
decisiontree_predict

# Calculating R2 score :

from sklearn.metrics import r2_score
r2 = r2_score(y_test,decisiontree_predict)
print("R2 score : ",r2)

# Calculating Adj. R2 score:

Adjr2_2 = 1 - (1-r)*(len(y_test)-1)/(len(y_test)-x_test.shape[1]-1)
print("Adj. R-Squared : {}".format(Adjr2_2))

ax = sns.kdeplot(y_test, color = "r", label = "Actual value ")
sns.kdeplot(decisiontree_predict, color = "g", label = "Predicted Values", ax = ax)
plt.legend()
plt.title('Decision Tree Regression')

"""The Accuracy of Decision Tree lies between 95-99%"""

# @title Cross-validation of Linear Regression
from sklearn.model_selection import cross_val_score
lr_model = linear_model.LinearRegression()
lr_model.fit(x_train,y_train)
accuracies = cross_val_score(estimator = lr_model, X = x_train, y=y_train, cv = 10)

a = (accuracies.mean()*100)
b = (accuracies.std()*100)

# Mean Accuracy and SD of 10 fold results

print("Accuracy : {:.2f}%".format (a))
print("Standard Deviation : {:.2f}%".format(b))

# @title Cross-validation of Random Forest
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = random_model, X = x_train, y=y_train, cv = 10)

a1 = (accuracies.mean()*100)
b1 = (accuracies.std()*100)

# Mean Accuracy and SD of 10 fold results

print("Accuracy : {:.2f}%".format (accuracies.mean()*100))
print("Standard Deviation : {:.2f}%".format(accuracies.std()*100))

# Calculating Errors
from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(rf_predict,y_test))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, rf_predict))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, rf_predict)))

# @title Cross-validation of Decision Tree
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = dec_model, X = x_train, y=y_train)

a2 = (accuracies.mean()*100)
b2 = (accuracies.std()*100)

# Accuracy and Standard Deviation
print("Accuracy : {:.2f}%".format (accuracies.mean()*100))
print("Standard Deviation : {:.2f}%".format(accuracies.std()*100))

# @title Comparing Predicting values of algorithms
ax1 = sns.kdeplot(lr_predict, color = "r", label = "Linear Regression")
ax2 = sns.kdeplot(decisiontree_predict, color = "g", label = "Decision Tree")
ax3 = sns.kdeplot(rf_predict, color = "b", label = "Random Forest")
plt.legend()
plt.title('Comparing Predicting values of algorithms')

# importing required libraries
import numpy as np
import matplotlib.pyplot as plt

# create a dataset
Algorithms = ['Linear Regression','Random Forest', 'Decision-tree']
Accuracy = [a, a1, a2]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color=['blue', 'green','yellow'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('Accuracy(in %)')
plt.xlabel('Machine Learning Regression Techniques')

# Show graph
plt.show()

# importing required libraries
import numpy as np
import matplotlib.pyplot as plt

# create a dataset
Algorithms = ['Linear Regression','Random Forest', 'Decision-tree']
Accuracy = [b, b1, b2]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color=['blue', 'green','yellow'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('Standard Deviation(in %)')
plt.xlabel('Machine Learning Regression Techniques')

# Show graph
plt.show()
plt.savefig('SD.png')

import numpy as np
import matplotlib.pyplot as plt

# create a dataset
Algorithms = ['Linear Regression','Random Forest', 'Decision-tree']
Accuracy = [r, r1, r2]

x_pos = np.arange(len(Accuracy))

# Create bars with different colors
plt.bar(x_pos, Accuracy, color=['blue', 'green','yellow'])

# Create names on the x-axis
plt.xticks(x_pos, Algorithms)
plt.ylabel('R-Squared Score')
plt.xlabel('Machine Learning Regression Techniques')

# Show graph
plt.show()
plt.savefig('SD.png')

# Importing required libraries
import numpy as np
import matplotlib.pyplot as plt

# set width of bar
barWidth = 0.25
fig = plt.subplots(figsize =(8, 5))

# set height of bar
Algorithms = ['Linear Rehression','Random Forest', 'Decision-tree']
Accuracy = [a, a1, a2]
Standard_Deviation = [b, b1,b2]

# Set position of bar on X axis
br1 = np.arange(len(Accuracy))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]

# Make the plot
plt.bar(br1, Accuracy, color ='green', width = barWidth,
        edgecolor ='grey', label ='Accuracy')
plt.bar(br2, Standard_Deviation, color ='yellow', width = barWidth,
        edgecolor ='grey', label ='Standard Devation')

# Adding Xticks
plt.xlabel('Algorithms', fontweight ='bold', fontsize = 10)
plt.ylabel('Accuracy (in %)', fontweight ='bold', fontsize = 10)
plt.xticks([r + barWidth for r in range(len(Accuracy))],
        Algorithms)

plt.legend()
plt.show()

# Mean Accuracy and SD of 10 fold results
print("Linear Regression Model:")
print("\tAccuracy : {:.2f}%".format (a))
print("\nDecision Tree Model:")
print("\tAccuracy : {:.2f}%".format (a2))
print("\nRandom Forrest Model:")
print("\tAccuracy : {:.2f}%".format (a1))

from sklearn.ensemble import RandomForestRegressor

# Define features and target again
X = df[['Dist Name', 'Crop', 'Area(1000 ha)', 'Total Rainfall', 'Avg Temp']]
y = df['Yield(Kg per ha)']

# Train Random Forest quickly
rf_model = RandomForestRegressor(n_estimators=20, max_depth=12, random_state=42, n_jobs=-1)
rf_model.fit(X, y)

print("✅ Random Forest model trained and ready for prediction")

# User input
dist_input = input("Enter District Name: ")
crop_input = input("Enter Crop Name: ")
rainfall_input = float(input("Enter Rainfall (mm): "))
temp_input = float(input("Enter Avg Temperature (°C): "))
area_input = float(input("Enter Area in 1000 hectares: "))

# Build dataframe directly with the correct feature order
user_data = pd.DataFrame([{
    'Dist Name': dist_input,
    'Crop': crop_input,
    'Area(1000 ha)': area_input,
    'Total Rainfall': rainfall_input,
    'Avg Temp': temp_input
}], columns=['Dist Name', 'Crop', 'Area(1000 ha)', 'Total Rainfall', 'Avg Temp'])

# Encode categories to match training
user_data['Dist Name'] = pd.Categorical(user_data['Dist Name'], categories=df['Dist Name'].unique()).codes
user_data['Crop'] = pd.Categorical(user_data['Crop'], categories=df['Crop'].unique()).codes

# Predict with trained model
predicted_yield = rf_model.predict(user_data)[0]
print(f"\n✅ Expected Crop Yield: {predicted_yield:.2f} Kg per ha")